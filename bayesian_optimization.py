import os
from argparse import ArgumentParser

import yaml
from bayes_opt import BayesianOptimization
from bayes_opt.logger import JSONLogger
from bayes_opt.event import Events
from bayes_opt.util import load_logs

args = None
script_to_run = None
config_file = None
parameters = {}
run_configs = []
run_number = 0


def get_parameter_value(key, value):
    """
    Get the parameter value from the parameter grid based on the selected index or value.

    Args:
        key (str): The parameter key.
        value: The selected index or actual value.

    Returns:
        value: The parameter value.
    """
    if len(parameters[key]) > 2 or (isinstance(parameters[key], list) and isinstance(parameters[key][0], str)):
        value = parameters[key][round(value)]

    return value


def is_float(num):
    """
   Check if a string can be converted to a floating-point number.

   Args:
       num (str): The string to check.

   Returns:
       bool: True if the string can be converted to a float, False otherwise.
   """
    try:
        float(num)
        return True
    except ValueError:
        return False


def is_int(num):
    """
    Check if a string can be converted to an integer.

    Args:
        num (str): The string to check.

    Returns:
        bool: True if the string can be converted to an integer, False otherwise.
    """
    try:
        int(num)
        return True
    except ValueError:
        return False


def get_last_version():
    """
    Get the path to the last version directory in the specified root directory.

    Returns:
        str or None: The path to the last version directory or None if not found.
    """
    items = os.scandir(os.path.join(args.default_root_dir, args.logs_dir))

    # Filter out only the directories from the list
    max_mtime = 0
    last_directory_version = None

    for item in items:
        if not os.path.isdir(item.path):
            continue

        item_mtime = item.stat().st_mtime
        if max_mtime < item_mtime:
            max_mtime = item_mtime
            last_directory_version = item.path

    return last_directory_version


def get_best_val_loss_from_ckpt(path):
    """
    Get the best validation loss from the checkpoint files in a directory.

    Args:
        path (str): The path to the directory containing checkpoint files.

    Returns:
        float: The best validation loss.
    """
    items = os.listdir(path)

    # Get only the checkpoints
    checkpoints = [ckpt for ckpt in items if ckpt.endswith(".pt")]

    best_val_loss = float('inf')

    for ckpt in checkpoints:
        loss = float(ckpt.split("=")[-1].split(".pt")[0])
        if loss < best_val_loss:
            best_val_loss = loss

    return best_val_loss


# Evaluation function for Bayesian Optimization
def evaluate_model(**kwargs):
    """
    Evaluate a model with specified parameters using a command-line script.

    This function takes keyword arguments representing model or data parameters, constructs a command-line argument
    string, and runs a Python script with those arguments. It then retrieves the best validation loss from the checkpoints
    generated by the script.

    Args:
        **kwargs (dict): Keyword arguments representing model parameters and their values.

    Returns:
        float: The negative of the best validation loss, suitable for maximization in optimization tasks.
    """
    run_parameters_args = f""
    for key, param_value in kwargs.items():
        run_parameters_args += f"--{key} {get_parameter_value(key, param_value)} "

    run_parameters_args = run_parameters_args.rstrip()

    print(f"running {script_to_run} with config {config_file} and parameters: {run_parameters_args}")
    os.system(
        f"python {script_to_run} fit --config {config_file} {run_parameters_args} --trainer.default_root_dir {args.default_root_dir}")
    # navigate to the trainer's default root dir, get the latest version, find the checkpoint and pick the best val_loss
    last_version = get_last_version()

    if last_version is None:
        raise Exception("Last directory version not found!")

    last_version = os.path.join(last_version, "checkpoints")

    # Return the negative accuracy to maximize (Bayesian Optimization expects a maximization problem)
    return -get_best_val_loss_from_ckpt(os.path.join(args.default_root_dir, last_version))


def get_hierarchy_keys(data, pbounds, current_key=""):
    for key, value in data.items():
        new_key = f"{current_key}.{key}" if current_key else key

        if isinstance(value, dict):
            get_hierarchy_keys(value, pbounds, new_key)
        else:
            values = [int(v) if is_int(v) else float(v) if is_float(v) else v for v in value.split(",")]

            if len(values) > 2:
                pbounds[new_key] = [0, len(values) - 1]
            else:
                # if there is a string, handle the params as indexes
                if isinstance(values[0], str):
                    if len(values) == 1:
                        pbounds[new_key] = [0, 0]
                    else:
                        pbounds[new_key] = [i for i in range(len(values))]
                else:
                    pbounds[new_key] = values
            parameters[new_key] = values


# Define the hyperparameter search space for Bayesian Optimization
def hyper_search_space(grid_file: str):
    """
    Define the hyperparameter search space for Bayesian Optimization based on a YAML grid file.

    Args:
        grid_file (str): Path to the YAML grid file specifying hyperparameter ranges.

    Returns:
        dict: A dictionary of hyperparameter bounds (pbounds) for Bayesian Optimization.
    """
    pbounds = {}

    with open(grid_file, "r") as f:
        data = yaml.safe_load(f)

        global script_to_run
        global parameters
        global config_file

        script_to_run = data["script"]
        config_file = data["config_file"]

        attr_keys = data["attr_keys"]

        get_hierarchy_keys(attr_keys, pbounds)

        return pbounds


if __name__ == "__main__":
    parser = ArgumentParser()

    parser.add_argument("--default_root_dir", type=str, default=os.getcwd())
    parser.add_argument("--logs_dir", type=str, default="lightning_logs")
    parser.add_argument("--grid_file", type=str, default="c3dp_grid.yaml")
    parser.add_argument("--n_iter", type=int, default=10)
    parser.add_argument("--n_init_points", type=int, default=5)
    parser.add_argument("--random_state", type=int, default=42)
    parser.add_argument("--bayesian_runs_export_file", type=str, default="bayesian_runs.json")
    parser.add_argument("--bayesian_runs_import_file", type=str, default=None)

    args = parser.parse_args()

    if not args.bayesian_runs_export_file or not args.bayesian_runs_export_file.endswith(".json"):
        raise Exception("Provide a valid JSON file for the `bayesian_runs_export_file` parameter")

    if args.bayesian_runs_import_file and args.bayesian_runs_import_file.endswith(".json"):
        raise Exception("Provide a valid JSON file for the `bayesian_runs_import_file` parameter")

    optimizer = BayesianOptimization(f=evaluate_model, pbounds=hyper_search_space(args.grid_file), verbose=2,
                                     random_state=args.random_state)

    logger = JSONLogger(path=args.bayesian_runs_export_file)
    optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)

    if args.bayesian_runs_import_file:
        load_logs(optimizer, logs=[args.bayesian_runs_import_file])
    else:
        optimizer.maximize(init_points=args.n_init_points, n_iter=args.n_iter)

    print("Best result: {}; f(x) = {}.".format(optimizer.max["params"], optimizer.max["target"]))
